# -*- coding: utf-8 -*-
"""nhóm 14

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OvNFRG1Kh290ltLgLl0LsLnIavwk81Mv

# Bot Hỗ Trợ Tuyển sinh đại học
"""

from google.colab import drive
drive.mount('/content/drive')

"""## RAG là gì?

**RAG** là viết tắt của **Retrieval Augmented Generation** (Tạo Tăng Cường Truy Xuất)


Rag được giới thiệu trong bài It was introduced in the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2005.11401).


**Mỗi bước có thể được chia nhỏ thành:**

* **Retrieval** - Tìm kiếm thông tin có liên quan từ một nguồn cho một truy vấn. Ví dụ: lấy các đoạn văn bản Wikipedia có liên quan từ cơ sở dữ liệu cho một câu hỏi.
* **Augmented** - Sử dụng thông tin có liên quan đã lấy được để sửa đổi đầu vào thành một mô hình tạo (ví dụ: LLM).
* **Generation** - Tạo đầu ra cho một đầu vào. Ví dụ: trong trường hợp của LLM, tạo một đoạn văn bản cho một lời nhắc nhập.

## Tại sao dùng RAG ?
Mục tiêu chính của RAG là cải tiến đầu ra của LLM.  

Có 2 Cải Tiến chính của Rag do với LLM thông thường:
1. **Ngăn Ngừa Hallucinations(ảo giác)**: LLM dễ gây ra ảo giác tiềm ẩn, tức là tạo ra thứ gì đó có vẻ đúng nhưng không phải vậy. RAG pipeline có thể giúp LLM tạo ra nhiều đầu ra thực tế hơn bằng cách cung cấp cho chúng các đầu vào thực tế (đã truy xuất). Và ngay cả khi câu trả lời được tạo ra từ đường ống RAG có vẻ không đúng, do quá trình truy xuất, bạn cũng có thể truy cập vào các nguồn mà câu trả lời đó đến từ đó.

2. **Hoạt động với Dữ Liệu Tùy Chỉnh:**  Nhiều LLM cơ bản được đào tạo bằng dữ liệu văn bản quy mô internet. Điều này có nghĩa là chúng có khả năng mô hình hóa ngôn ngữ tuyệt vời, tuy nhiên, chúng thường thiếu kiến ​​thức cụ thể. Hệ thống RAG có thể cung cấp cho LLM dữ liệu theo lĩnh vực cụ thể như thông tin y tế hoặc tài liệu của công ty và do đó tùy chỉnh đầu ra của chúng để phù hợp với các trường hợp sử dụng cụ thể.

## Vì sao áp dụng RAG cho Trợ Lý Ảo cho tuyển sinh đại học
Vì RAG có thể truy cứu dữ liệu 1 cách tổng thể và hiệu quả. Cho phép LLM hiểu được và ghi nhớ bối cảnh của bài viết, chống dẫn tới ảo giác và đứa ra những phản hồi tốt tới đầu vào tùy chỉnh.

## Thuật Ngữ Chính

| **Term** | **Description** |
|----|----|
| **Token** | 1 đoạn text, VD "life, is good!" có thể chia thành ["life", ",", "is", "good", "!"]. <br> Một token có thể là 1 từ hoàn chỉnh hoặc 1 phần của từ hoặc 1 ký hiệu câu. <br> 1 token ~ 4 ký tự trong Tiếng Anh, 100 token ~ 75 từ. <br> Văn bản được chia thành các token trước khi được chuyển đến LLM. |
| **Embedding** | Chuyển hóa dữ liệu text sang dữ liệu số cho máy có thể đọc. <br> Dữ liệu số thường thuộc kiểu int hoặc là float. <br> VD, "life is good" thành {1232: b'life', 242: b'is', 132: b'good'} dùng Byte pair encoding (mã hóa cặp Byte). <br> Google có thư viện tokenization là  [SentencePiece](https://github.com/google/sentencepiece) |
| **Embedding model** | Mô hình được thiết kế để nhận dữ liệu đầu vào và đưa ra 1 biểu diễn số. <br> VD, 1 mô hình nhúng văn bản có thể lấy 384 token và biến nó thành 1 vector kích thước 768 (768 hàng). <br> note: 1 thường khác mô hình LLM. |
| **Similarity search/vector search** | "Tìm kiếm tương đồng / Tìm kiếm vector" nhằm mục đích tìm 2 vector gần nhau trong không gian n chiều. <br> VD, 2 văn bản có chủ đề tương tự sẽ có giá trị các vector gần nhau, trong khi 2 đoạn văn bản về 2 chủ đề khác nhau sẽ có giá trị các vector thấp. <br> (Mình gọi "điểm tương tự" của 2 token là "giá trị các vector"). <br> Các phép đo điểm tương tự phổ biến là tích vô hướng và độ tương tự cosin. |
| **Large Language Model (LLM)** | Mô hình học máy được đào tạo sâu có khả năng hiểu và tạo văn bản ngôn ngữ con người. <br> Nói sâu hơn, đây là 1 mô hình đc đào tạo để biểu diễn các mẫu (pattern) trong văn bản theo dạng số. <br> 1 LLM sinh sẽ sinh ra 1 chuỗi số khi đc cung cấp 1 chuỗi số (chuỗi số thường là đoạn văn bản sau khi embedding). <br> VD: được cung cấp 1 chuỗi văn bản "life is good", 1 LLM có thể tạo ra "I want to play today". <br> Mô hình dự đoán này phụ thuộc nhiều vào dữ liệu Huấn Luyện và Yêu Cầu của người dùng (User Prompt). |
| **LLM context window** | Số Lượng Tokens mà LLM có thể nhận. <br> VD, vào tháng 8 năm 2024 GPT-4o có Context Window mặc định là 8192 nghìn tokens. (khoảng 96 trang văn bản word). <br> Mô hình Gemma-7b-it mình dùng trong dự án (tháng 8 năm 2024) này có Context Window là 2048 tới 4096 token (24 trang văn bản). <br> Context Window hơn nghĩa là LLM (hoặc RAG pipeline) có thể nhận nhiều thông tin có liên quan hơn để hỗ trợ truy vấn mà không gây ra ảo giác cao. |
| **Prompt** | Một thuật ngữ để mô tả đầu vào cho LLM tạo sinh. Ý tưởng của "kỹ thuật nhắc nhở" [prompt engineering](https://www.google.com/url?q=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FPrompt_engineering) <br> là cấu trúc đầu vào dựa trên văn bản (hoặc dựa trên hình ảnh) cho LLM tạo sinh theo 1 cách cự thể để đầu ra được tạo ra là lý tưởng vs người gửi. <br> Kỹ thuật này có thể thực hiện được vì khả năng học trong ngữ cảnh của LLM, <br> tức là nó có thể sử dụng cách biểu diễn ngôn ngữ của mình để phân tích nhắc và nhận ra đầu vào phù hợp có thể là gì. |

## Chúng ta sẽ xây dựng những gì ?

Chúng ta sẽ xây dựng đường ống RAG cho phép chúng ta trò chuyện với một tài liệu PDF về thông tin tuyển sinh.

Các phần ta sẽ Code:
1. Mở file PDF
2. Định dạng văn bản của file PDF để chuẩn bị cho mô hình nhúng (quy trình này gọi là Chunking phân tách/phân đoạn văn bản)
3. Nhúng tất cả các đoạn văn bản và chuyển chúng thành biểu diễn số (vector) để lưu trữ sử dụng sau.
4. Xây dựng 1 hệ thống truy xuất sử dụng tìm kiếm vector để tìm các đoạn văn bản có liên quan dựa trên truy vấn
5. Tạo lời nhắc (prompt) kết hợp các đoạn văn bản đã truy xuất.
6. Tạo câu trả lời cho truy vấn xuất dựa trên đoạn trích từ PDF.

Các Bước trên có thể chia thành 2 phần chính:
1. "Tiền Xử Lý / Nhúng" tài liệu (bước 1 - 3).
2. Tìm và trả lời câu hỏi (bước 4 - 6).

Đây là cấu trúc chúng ta sẽ đi theo:
<img src="https://docs.aws.amazon.com/images/sagemaker/latest/dg/images/jumpstart/jumpstart-fm-rag.jpg" alt="flowchart of a local RAG workflow" />

Tài liệu tham khảo của cấu trúc trên: https://developer.nvidia.com/blog/rag-101-demystifying-retrieval-augmented-generation-pipelines/

## Tải Các Yêu Cầu và Thiết Lập Môi Trường
"""

!sudo apt-get update -y
!sudo apt-get install python3.11 python3.11-dev python3.11-distutils libpython3.11-dev

# Perform Google Colab installs (if running in Google Colab)
import os

if "COLAB_GPU" in os.environ:
    print("[INFO] Running in Google Colab, installing requirements.")
    !pip install -U torch # requires torch 2.1.1+ (for efficient sdpa implementation)
    !pip install PyMuPDF # for reading PDFs with Python
    !pip install tqdm # for progress bars
    !pip install sentence-transformers # for embedding models
    !pip install accelerate # for quantization model loading
    !pip install bitsandbytes # for quantizing models (less storage space)
    !pip install flash-attn --no-build-isolation # for faster attention mechanism = faster LLM inference

"""# 1. Xử lý và nhúng tài liệu/văn bản
Nguyên Liệu:

* Tài Liệu PDF tùy chọn.
* Mô hình Nhúng tùy chọn.

Các Bước:

* Import tài liệu PDF .
* Xử lý văn bản để nhúng (e.g. chia văn bản thành các câu).
* Nhúng các đoạn văn bản vào mô hình nhúng.
* Lưu trữ vào tệp để sử dụng sau.

### Tải Văn Bản PDF
Chúng ta sẽ dùng 1 bộ data về tuyển sinh đại học
"""

!pip install bs4

# Download PDF file
import os
import requests

# Get PDF document
pdf_path = "/content/drive/MyDrive/AI_DATA/Thông Tin Tuyển Sinh.pdf"

# Download PDF if it doesn't already exist
if not os.path.exists(pdf_path):
  print("File doesn't exist")
else:
  print(f"File {pdf_path} exists.")

!pip install fitz
!pip install --force-reinstall pymupdf

# Run this is you got ModuleNotFoundError: No module named 'frontend'
!pip install --force-reinstall pymupdf

"""Sau khi có được file PDF mình sẽ dùng thư viện fitz để mở và xem qua file PDF để hiểu hơn về nó."""

import fitz # (pymupdf, found this is better than pypdf for our use case)
from tqdm.auto import tqdm # for progress bars visualization, requires !pip install tqdm

def text_formatter(text: str) -> str:
    """Performs minor formatting on text."""
    cleaned_text = text.replace("\n", " ").strip() # strip "line break" with a "space"

    return cleaned_text

# Open PDF and get lines/pages
# Note: this only focuses on text, rather than images/figures etc
def open_and_read_pdf(pdf_path: str) -> list[dict]:
    """
    Opens a PDF file, reads its text content page by page, and collects statistics.

    Parameters:
        pdf_path (str): The file path to the PDF document to be opened and read.

    Returns:
        list[dict]: A list of dictionaries, each containing the page number
        (adjusted), character count, word count, sentence count, token count, and the extracted text
        for each page.
    """
    doc = fitz.open(pdf_path)  # open a document
    pages_and_texts = []
    for page_number, page in tqdm(enumerate(doc)):  # iterate the document pages
        text = page.get_text()  # get plain text encoded as UTF-8
        text = text_formatter(text)
        pages_and_texts.append({"page_number": page_number  ,
                                "page_char_count": len(text),
                                "page_word_count": len(text.split(" ")),
                                "page_sentence_count_raw": len(text.split(". ")),
                                "page_token_count": len(text) / 4,  # 1 token = ~4 chars, see: https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them
                                "text": text})
    return pages_and_texts

pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)
pages_and_texts[:3]

"""lấy 1 tập mẫu ngẫu nhiên để trực quan hóa"""

import random

random.sample(pages_and_texts, k=3) # return 3 random pages

"""### Lấy số liệu thống kê về văn bản
Điều này có nghĩa là mô hình đã được đào tạo để thu thập và chuyển thành văn bản nhúng với 800 tokens (1 tokens ~= 4 ký tự ~= 0,75 từ).
"""

import pandas as pd

df = pd.DataFrame(pages_and_texts) # Frame the Data
df.head() # Check few first data

# Token Count by Page Number

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.plot(df['page_number'], df['page_token_count'])
plt.xlabel('Page Number')
plt.ylabel('Token Count')
_ = plt.title('Token Count by Page Number')

# Get stats
df.describe().round(2)

"""**Mô tả trên cho chúng ta biết:** thông tin về số trang, ký tự, từ, số token từng trang trên tổng số trang là 13.

+ count: Số lượng trang không phải giá trị null trong mỗi cột
+ mean: Giá trị trung bình (mean) của mỗi cột.
+ std: Độ lệch chuẩn, đo lường lượng biến thiên hoặc độ phân tán.
+ min: Giá trị nhỏ nhất trong mỗi cột.
+ 25%: 25% trên tổng (còn được gọi là tứ phân vị thứ nhất), biểu thị rằng 25% điểm dữ liệu nhỏ hơn hoặc bằng giá trị này.
+ 50%: 50% trên tổng (còn được gọi là trung vị), biểu thị rằng 50% điểm dữ liệu nhỏ hơn hoặc bằng giá trị này.
+ 75%: 75% trên tổng (còn được gọi là tứ phân vị thứ ba), biểu thị rằng 75% điểm dữ liệu nhỏ hơn hoặc bằng giá trị này.
+ max: Giá trị lớn nhất trong từng cột.

Tải thư viện từ điển Tiếng Việt bằng spacy để có thể phân biệt các từ trong văn bản.
"""

!pip install pyvi

!pip install https://gitlab.com/trungtv/vi_spacy/-/raw/master/packages/vi_core_news_lg-3.6.0/dist/vi_core_news_lg-3.6.0.tar.gz

#phiên bản numpy phù hợp
!pip uninstall numpy
!pip install numpy==1.26.4

"""### Xử lý văn bản (Chia các trang thành các câu)

Trong phần , mình sẽ phá từng văn bản thành các, trung bình 1 trang có thể chia thành 5, 7 hoặc 10 câu.
Chúng ta thực hiện các bước sau:

`Nhận văn bản -> Chia thành các nhóm/chunks -> nhúng các nhóm/chunks -> dùng để nhúng`

Có 2 cách để chia văn bản thành các câu:
1. Dùng `text = .split("")` của python
2. Dùng thư viện spaCy hoặc nltk.

Vì sao phải chia thành các ?
+ Để xử lý nhanh gọn các văn bản lớn chứa nhiều ký tự.
+ Để có thể truy xuất từng nhóm các câu hiệu quả trong RAG pipeline.
"""

import spacy

# Assuming a valid Vietnamese model is installed (e.g., a custom one if vi_core_news_lg isn't real)
nlp = spacy.load('vi_core_news_lg')  # Replace with a real model if needed

# Add the sentencizer pipeline
nlp.add_pipe("sentencizer")

# Preprocess the text by adding a space after the comma
text = "Đây là một câu.Đây là một câu khác"  # Added space
doc = nlp(text)

# Check the number of sentences
sentences = list(doc.sents)
assert len(sentences) == 2, f"Expected 2 sentences, got {len(sentences)}"

# Print the sentences
for i, sent in enumerate(sentences):
    print(f"Sentence {i+1}: {sent.text}")

for item in tqdm(pages_and_texts): # tqlm to display loop as progress bar
    item["sentences"] = list(nlp(item["text"]).sents) # text into "sentences"

    # Make sure all sentences are strings
    item["sentences"] = [str(sentence) for sentence in item["sentences"]] # convert sentence to string

    # Count the sentences
    item["page_sentence_count_spacy"] = len(item["sentences"])

print(pages_and_texts[4]["sentences"])

# Inspect an example
random.sample(pages_and_texts, k=1) # chọn random 1 trang

"""Mô tả trang vừa lấy được cho mình hiểu được độ phức tạp của 1 trang pdf và để giải quyết sự phức đó chúng ta có thể dùng chunking"""

df = pd.DataFrame(pages_and_texts)
df.describe().round(2)

"""Thư viện spaCy chia từng câu bởi các dấu cách (vd: "," và ".")

Chúng ta đã chia thành câu văn bản thành các câu, tiếp theo ta sẽ gộp các câu đó lại thành các cụm/.

### Chia nhỏ các câu (Chunking)

Vì sao:
* Để dễ dàng quản lý các đoạn văn bản có kích thước tương tự.
* Không làm quá tải mô hình nhúng với các tokens   (note: nếu mô hình có khả năng nhận 384 tokens, mình có thể sẽ mất 1 số thông tin nếu cố nhúng 1 chuỗi hơn 400 tokens)
* Cửa sổ ngữ cảnh (Context windown - số lượng token LLM có thể nhận) có thể bị giới hạn và yêu cầu sức mạnh tính toán lớn hơn nên chúng ta muốn đảm bảo sự ổn định của mô hình.
"""

# Define split size to turn groups of sentences into chunks
num_sentence_chunk_size = 8

def split_list(input_list: list, slice_size: int) -> list[list[str]]:
    """
    Splits the input_list into sublists of size slice_size (or as close as possible).

    Parameters:
    - input_list: The list to be split into chunks.
    - slice_size: The number of items in each chunk.

    Returns:
    - A list of sublists, where each sublist contains up to slice_size elements.

    Example:
    A list of 17 sentences with a slice_size of 8 would be split into:
    [[sentence1, ..., sentence8], [sentence9, ..., sentence16], [sentence17]]
    """
    # Create a list of sublists, each with a maximum of slice_size elements
    return [
        # Summarize: For all sentences in a page, every 8 senteces -> slice 8 sentence as a chunks.

        # Create a chunk from the current start_index to start_index + slice_size
        input_list[start_index : start_index + slice_size] # ex: 0: 0+8, 8: 8+8, 24: 24 + 8

        # Iterate over the input list with step size equal to slice_size
        for start_index in range(0, len(input_list), slice_size) # ex: for 1 in range(start: 0, end: 9, step: 8)
    ]

# Loop through each page and its associated text data
for item in tqdm(pages_and_texts):
    # Split the sentences on the current page into chunks
    item["sentence_chunks"] = split_list(
        input_list=item["sentences"], # sentence of a page
        slice_size=num_sentence_chunk_size # chunk size
    )

    # Store the number of chunks created
    item["num_chunks"] = len(item["sentence_chunks"])

pages_and_texts[12]['text']

# Sample an example from the group (note: many pages have only 1 chunk as they have <=8 sentences total)
random.sample(pages_and_texts, k=1)

# Create a DataFrame to get stats
df = pd.DataFrame(pages_and_texts)
df.describe().round(2)

"""### Chia từng phần thành một mục riêng ( Tokenizer the each Sentence)

Ở trên thư viện spaCy đã giúp chia trang thành các câu, trong phần này mình sẽ tạo gộp 10 câu thành 1 nhóm cho từng từng
"""

import re

# Split each chunk into its own item
pages_and_chunks = [] # list of pages contain chunks
for item in tqdm(pages_and_texts):
    for sentence_chunk in item["sentence_chunks"]: # store the processed chunk in dict with key "sentence_chunks"
        chunk_dict = {} # store chunks of a page
        chunk_dict["page_number"] = item["page_number"] # Stores the page number associated with this chunk.

        # Join the sentences together into a paragraph-like structure, aka a chunk (so they are a single string)

        # remove double space with single space and join them in a single string
        joined_sentence_chunk = "".join(sentence_chunk).replace("  ", " ").strip()
        # ".A" -> ". A" for any full-stop/capital letter combo. (for spacy to recognize stop word better)
        joined_sentence_chunk = re.sub(r'\.([A-Z])', r'. \1', joined_sentence_chunk)
        chunk_dict["sentence_chunk"] = joined_sentence_chunk # Stores the joined sentence chunk.

        # Get stats about the chunk
        chunk_dict["chunk_char_count"] = len(joined_sentence_chunk)
        chunk_dict["chunk_word_count"] = len([word for word in joined_sentence_chunk.split(" ")])
        chunk_dict["chunk_token_count"] = len(joined_sentence_chunk) / 4 # 1 token = ~4 characters

        pages_and_chunks.append(chunk_dict)

# How many chunks do we have?
len(pages_and_chunks)

"""Let see the Chunk we have"""

# View a random sample
random.sample(pages_and_chunks, k=1)

# Get stats about our chunks
df = pd.DataFrame(pages_and_chunks)
df.describe().round(2)

"""### Lọc số câu với Token thấp
Mình sẽ muốn kiểm tra các Chunk có số token thấp (khả năng là noise)

-> Bằng cách xem các trang với ít hơn 30 tokens (độ vài trung bình của 1 câu) và mình thấy rẳng đây là các phần đầu câu (header) và cuối câu (footer) không mang lại nhiều giá trị và không đáng giữ lại.
"""

# Show random chunks with under 30 tokens in length
min_token_length = 30
filtered_df = df[df["chunk_token_count"] <= min_token_length]

# Check if the filtered DataFrame is empty before sampling
if not filtered_df.empty:
    for row in filtered_df.sample(min(5, len(filtered_df))).iterrows():
        print(f'Chunk token count: {row[1]["chunk_token_count"]} | Text: {row[1]["sentence_chunk"]}')
else:
    print(f"No chunks found with token count less than or equal to {min_token_length}.")

"""Mình sẽ lọc DatDataFrame/danh sách của từ điển (của model) chỉ bao gồm tokens có độ dài trên 30.   """

pages_and_chunks_over_min_token_len = df[df["chunk_token_count"] > min_token_length].to_dict(orient="records")
pages_and_chunks_over_min_token_len[:2] # VIew 2 page with more than 30 tokens

"""### Nhúng các đoạn văn bản
Trong khi con người hiểu văn bản tốt, máy chỉ hiểu tốt nhất khi văn bản ở dưới dạng số.

Embedding nói đơn giản nó là nhúng các tokens dạng text về dạng số.

Điểm mạnh của Embedding hiện đại được gọi là *biểu diễn có thể học được*

Nghĩa là thay vì chúng chỉ ánh xạ các từ/tokens/ký tự thành số (ví dụ: {"a": 0, "b": 1, "c": 3...}), các biểu diễn số của tokens được học bằng cách xem các tập văn bản lớn và hiểu được các token liên kết với nhau như thế nào.

Để đạt được điều đó ta có thể chuyển chuyển các chunk thành các vector nhúng.


"""

!pip install --upgrade torch torchvision transformers

# Requires !pip install sentence-transformers
from sentence_transformers import SentenceTransformer

embedding_model = SentenceTransformer(model_name_or_path="all-mpnet-base-v2",
                                      device="cpu") # choose the device to load the model to (note: GPU will often be *much* faster than CPU)

# Tạo danh sách các câu liên quan đến tuyển sinh bằng tiếng Việt
sentences = [
    "Thư viện Hướng dẫn Tuyển sinh cung cấp một cách dễ dàng và mã nguồn mở để hiểu quy trình tuyển sinh.",
    "Hồ sơ có thể được nộp từng cái một hoặc dưới dạng danh sách các tài liệu.",
    "Tiêu chí tuyển sinh là một trong những yếu tố quan trọng nhất khi chọn trường đại học!",
    "Học cách chuẩn bị hồ sơ tốt và bạn sẽ tiến gần hơn đến việc trở thành một sinh viên thành công."
]

# Sentences are encoded/embedded by calling model.encode()
embeddings = embedding_model.encode(sentences)
embeddings_dict = dict(zip(sentences, embeddings))

# See the embeddings
for sentence, embedding in embeddings_dict.items():
    print("Sentence:", sentence)
    print("Embedding:", embedding)
    print("")

"""Thử với 1 câu"""

single_sentence = "Yo! Xin Chào"
single_embedding = embedding_model.encode(single_sentence)
print(f"Sentence: {single_sentence}")
print(f"Embedding:\n{single_embedding}")
print(f"Embedding size: {single_embedding.shape}")

"""Vector nhúng của mình có dạng `(768, )` , là ta có 768 số biểu diễn cho văn bản của chúng ta trong không gian nhiều.


> Note: để duy trì độ ổn định khi nhân vector mình sẽ duy trì với 1 chiều không gian là 768, bất kể 1 câu có độ dài 1 hay 1000 tokens, nó sẽ được cắt hoặc đệm thêm 0 để về 384 chiều và chuyển sang vector nhúng có dạng (768, ).



"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Uncomment to see how long it takes to create embeddings on CPU
# # Make sure the model is on the CPU
# embedding_model.to("cpu")
# 
# # Embed each chunk one by one
# for item in tqdm(pages_and_chunks_over_min_token_len):
#     item["embedding"] = embedding_model.encode(item["sentence_chunk"])

"""dùng `%%time` để xem ta mất bao nhiêu thời gian để nhúng."""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Send the model to the GPU
# embedding_model.to("cuda") # requires a GPU installed, for reference on my local machine, I'm using a NVIDIA RTX 4090
# 
# # Create embeddings one by one on the GPU
# for item in tqdm(pages_and_chunks_over_min_token_len):
#     item["embedding"] = embedding_model.encode(item["sentence_chunk"])

"""Ta thấy GPU xử lý nhanh hơn 20 lần CPU"""

# Turn text chunks into a single list
text_chunks = [item["sentence_chunk"] for item in pages_and_chunks_over_min_token_len]

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Embed all texts in batches
# text_chunk_embeddings = embedding_model.encode(text_chunks,
#                                                batch_size=25, # you can use different batch sizes here for speed/performance, I found 25 works well for this use case
#                                                convert_to_tensor=True) # optional to return embeddings as tensor instead of array
# 
# text_chunk_embeddings

"""### Lưu Vector Nhúng vào File
Vì việc Nhúng mất rất nhiều thời gian, mình sẽ lưu và chuyển nó về danh sách từ điển `text_chunks_and_embeddings_df`  
"""

# Save embeddings to file
text_chunks_and_embeddings_df = pd.DataFrame(pages_and_chunks_over_min_token_len)
embeddings_df_save_path = "/content/drive/MyDrive/AI_DATA/text_chunks_and_embeddings_df.csv"
text_chunks_and_embeddings_df.to_csv(embeddings_df_save_path, index=False)

"""Xem Chunk dưới dạng Vector Nhúng"""

# Import saved file and view
text_chunks_and_embedding_df_load = pd.read_csv(embeddings_df_save_path)
text_chunks_and_embedding_df_load.head()

import random

import torch
import numpy as np
import pandas as pd

device = "cuda" if torch.cuda.is_available() else "cpu"

# Import texts and embedding df
text_chunks_and_embedding_df = pd.read_csv("text_chunks_and_embeddings_df.csv")

# Convert embedding column back to np.array (it got converted to string when it got saved to CSV)
text_chunks_and_embedding_df["embedding"] = text_chunks_and_embedding_df["embedding"].apply(lambda x: np.fromstring(x.strip("[]"), sep=" "))

# Convert texts and embedding df to list of dicts
pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient="records")

# Convert embeddings to torch tensor and send to device (note: NumPy arrays are float64, torch tensors are float32 by default)
embeddings = torch.tensor(np.array(text_chunks_and_embedding_df["embedding"].tolist()), dtype=torch.float32).to(device)
embeddings.shape

embeddings[0]

"""# Mô Hình Nhúng
Được chọn dựa trên: https://huggingface.co/spaces/mteb/leaderboard

Mô hình nhúng của chúng ta chuyển 384 tokens về vector nhúng kích thước 768.

Có kích thước khá bé (~420MB) và tiết kiệm năng lượng tính toán.  

Mô hình được chọn dựa trên các tiêu chí:  
+ Kích thước đầu vào: nhỏ
+ Kích thước vector : vector nhúng nhỏ cho số lượng token dưới 1000.
+ Kích thước mô hình: nhỏ để tối ưu thời gian xử lý các đầu vào và đầu ra ngắn.
+ Mô hình mở hay đóng: sử dụng mô hình đóng để tiết kiệm tài nguyên và chỉ cần gọi API nên rất tiện lợi cho việc sử dụng
"""

from sentence_transformers import util, SentenceTransformer

# Choose an embedding model (not the best but good enough)
embedding_model = SentenceTransformer(model_name_or_path="all-mpnet-base-v2",
                                      device=device) # choose the device to load the model to

# 1. Define the query
# Note: This could be anything. But since we're working with a nutrition textbook, we'll stick with nutrition-based queries.
query = "Tuyển sinh đại học"
print(f"Query: {query}")

# 2. Embed the query to the same numerical space as the text examples
# Note: It's important to embed your query with the same model you embedded your examples with.
query_embedding = embedding_model.encode(query, convert_to_tensor=True)

# 3. Get similarity scores with the dot product (we'll time this for fun)
from time import perf_counter as timer

start_time = timer()
dot_scores = util.dot_score(a=query_embedding, b=embeddings)[0]
end_time = timer()

print(f"Time take to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.")

# 4. Get the top-k results (we'll keep this to 5)
top_results_dot_product = torch.topk(dot_scores, k=5)
top_results_dot_product

larger_embeddings = torch.randn(100*embeddings.shape[0], 768).to(device)
print(f"Embeddings shape: {larger_embeddings.shape}")

# Perform dot product across 168,000 embeddings
start_time = timer()
dot_scores = util.dot_score(a=query_embedding, b=larger_embeddings)[0]
end_time = timer()

print(f"Time take to get scores on {len(larger_embeddings)} embeddings: {end_time-start_time:.5f} seconds.")

# Define helper function to print wrapped text
import textwrap

def print_wrapped(text, wrap_length=80):
    wrapped_text = textwrap.fill(text, wrap_length)
    print(wrapped_text)

print(f"Query: '{query}'\n")
print("Results:")
# Loop through zipped together scores and indicies from torch.topk
for score, idx in zip(top_results_dot_product[0], top_results_dot_product[1]):
    print(f"Score: {score:.4f}")
    # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)
    print("Text:")
    print_wrapped(pages_and_chunks[idx]["sentence_chunk"])
    # Print the page number too so we can reference the textbook further (and check the results)
    print(f"Page number: {pages_and_chunks[idx]['page_number']}")
    print("\n")

import fitz

# Open PDF and load target page
pdf_path = "/content/drive/MyDrive/AI_DATA/Thông Tin Tuyển Sinh.pdf" # requires PDF to be downloaded
doc = fitz.open(pdf_path)
current_page = 2
page = doc.load_page(current_page)

# Get the image of the page
img = page.get_pixmap(dpi=300)

# Optional: save the image
#img.save("output_filename.png")
doc.close()

# Convert the Pixmap to a numpy array
img_array = np.frombuffer(img.samples_mv,
                          dtype=np.uint8).reshape((img.h, img.w, img.n))

# Display the image using Matplotlib
import matplotlib.pyplot as plt
plt.figure(figsize=(13, 10))
plt.imshow(img_array)
plt.title(f"Query: '{query}' | Most relevant page:")
plt.axis('off') # Turn off axis
plt.show()

import torch

def dot_product(vector1, vector2):
    return torch.dot(vector1, vector2)

def cosine_similarity(vector1, vector2):
    dot_product = torch.dot(vector1, vector2)

    # Get Euclidean/L2 norm of each vector (removes the magnitude, keeps direction)
    norm_vector1 = torch.sqrt(torch.sum(vector1**2))
    norm_vector2 = torch.sqrt(torch.sum(vector2**2))

    return dot_product / (norm_vector1 * norm_vector2)

# Example tensors
vector1 = torch.tensor([1, 2, 3], dtype=torch.float32)
vector2 = torch.tensor([1, 2, 3], dtype=torch.float32)
vector3 = torch.tensor([4, 5, 6], dtype=torch.float32)
vector4 = torch.tensor([-1, -2, -3], dtype=torch.float32)

# Calculate dot product
print("Dot product between vector1 and vector2:", dot_product(vector1, vector2))
print("Dot product between vector1 and vector3:", dot_product(vector1, vector3))
print("Dot product between vector1 and vector4:", dot_product(vector1, vector4))

# Calculate cosine similarity
print("Cosine similarity between vector1 and vector2:", cosine_similarity(vector1, vector2))
print("Cosine similarity between vector1 and vector3:", cosine_similarity(vector1, vector3))
print("Cosine similarity between vector1 and vector4:", cosine_similarity(vector1, vector4))

def retrieve_relevant_resources(query: str,
                                embeddings: torch.tensor,
                                model: SentenceTransformer=embedding_model,
                                n_resources_to_return: int=5,
                                print_time: bool=True):
    """
    Embeds a query with model and returns top k scores and indices from embeddings.
    """

    # Embed the query
    query_embedding = model.encode(query,
                                   convert_to_tensor=True)

    # Get dot product scores on embeddings
    start_time = timer()
    dot_scores = util.dot_score(query_embedding, embeddings)[0]
    end_time = timer()

    if print_time:
        print(f"[INFO] Time taken to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.")

    scores, indices = torch.topk(input=dot_scores,
                                 k=n_resources_to_return)

    return scores, indices

def print_top_results_and_scores(query: str,
                                 embeddings: torch.tensor,
                                 pages_and_chunks: list[dict]=pages_and_chunks,
                                 n_resources_to_return: int=5):
    """
    Takes a query, retrieves most relevant resources and prints them out in descending order.

    Note: Requires pages_and_chunks to be formatted in a specific way (see above for reference).
    """

    scores, indices = retrieve_relevant_resources(query=query,
                                                  embeddings=embeddings,
                                                  n_resources_to_return=n_resources_to_return)

    print(f"Query: {query}\n")
    print("Results:")
    # Loop through zipped together scores and indicies
    for score, index in zip(scores, indices):
        print(f"Score: {score:.4f}")
        # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)
        print_wrapped(pages_and_chunks[index]["sentence_chunk"])
        # Print the page number too so we can reference the textbook further and check the results
        print(f"Page number: {pages_and_chunks[index]['page_number']}")
        print("\n")

query = "Học bổng"

# Get just the scores and indices of top related results
scores, indices = retrieve_relevant_resources(query=query,
                                              embeddings=embeddings)
scores, indices

# Print out the texts of the top scores
print_top_results_and_scores(query=query,
                             embeddings=embeddings)

!nvidia-smi

# Get GPU available memory
import torch
gpu_memory_bytes = torch.cuda.get_device_properties(0).total_memory
gpu_memory_gb = round(gpu_memory_bytes / (2**30))
print(f"Available GPU memory: {gpu_memory_gb} GB")

# Note: the following is Gemma focused, however, there are more and more LLMs of the 2B and 7B size appearing for local use.
if gpu_memory_gb < 5.1:
    print(f"Your available GPU memory is {gpu_memory_gb}GB, you may not have enough memory to run a Gemma LLM locally without quantization.")
elif gpu_memory_gb > 6:
    print(f"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in 4-bit precision.")
    use_quantization_config = True
    model_id = "google/gemma-2b-it"


print(f"use_quantization_config set to: {use_quantization_config}")
print(f"model_id set to: {model_id}")

"""### Connect and Verify Model from Hugging Face"""

from huggingface_hub import notebook_login

# Log in to Hugging Face (replace 'YOUR_TOKEN' with your actual token)
notebook_login()

# Install these if not already
# !pip install bitsandbytes
# !pip install huggingface_hub

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers.utils import is_flash_attn_2_available

# 1. Create quantization config for smaller model loading
# Requires !pip install bitsandbytes accelerate, see: https://github.com/TimDettmers/bitsandbytes, https://huggingface.co/docs/accelerate/
# For models that require 4-bit quantization (use this if you have low GPU memory available)
from transformers import BitsAndBytesConfig
quantization_config = BitsAndBytesConfig(load_in_4bit=False,
                                         bnb_4bit_compute_dtype=torch.float16)

# Bonus: Setup Flash Attention 2 for faster inference, default to "sdpa" or "scaled dot product attention" if it's not available
# Flash Attention 2 requires NVIDIA GPU compute capability of 8.0 or above, see: https://developer.nvidia.com/cuda-gpus
# Requires !pip install flash-attn, see: https://github.com/Dao-AILab/flash-attention
if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):
  attn_implementation = "flash_attention_2"
else:
  attn_implementation = "sdpa"
print(f"[INFO] Using attention implementation: {attn_implementation}")

# 2. Pick a model we'd like to use (this will depend on how much GPU memory you have available)
#model_id = "google/gemma-7b-it"
model_id = "ricepaper/vi-gemma-2b-RAG"
print(f"[INFO] Using model_id: {model_id}")

# 3. Instantiate tokenizer (tokenizer turns text into numbers ready for the model)
tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)

# 4. Instantiate the model
llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id,
                                                 torch_dtype=torch.float16, # datatype to use, we want float16
                                                 quantization_config=quantization_config if use_quantization_config else None,
                                                 low_cpu_mem_usage=True, # use full memory
                                                 attn_implementation=attn_implementation, # which attention version to use
                                                 device_map="auto" ) # Let Transformers handle device placement

if not use_quantization_config: # quantization takes care of device setting automatically, so if it's not used, send model to GPU
    llm_model.to("cuda")

llm_model

def get_model_num_params(model: torch.nn.Module):
    return sum([param.numel() for param in model.parameters()])

get_model_num_params(llm_model)

def get_model_mem_size(model: torch.nn.Module):
    """
    Get how much memory a PyTorch model takes up.

    See: https://discuss.pytorch.org/t/gpu-memory-that-model-uses/56822
    """
    # Get model parameters and buffer sizes
    mem_params = sum([param.nelement() * param.element_size() for param in model.parameters()])
    mem_buffers = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])

    # Calculate various model sizes
    model_mem_bytes = mem_params + mem_buffers # in bytes
    model_mem_mb = model_mem_bytes / (1024**2) # in megabytes
    model_mem_gb = model_mem_bytes / (1024**3) # in gigabytes

    return {"model_mem_bytes": model_mem_bytes,
            "model_mem_mb": round(model_mem_mb, 2),
            "model_mem_gb": round(model_mem_gb, 2)}

get_model_mem_size(llm_model)

input_text = "Tuyển sinh đại học"
print(f"Input text:\n{input_text}")

# Create prompt template for instruction-tuned model
dialogue_template = [
    {"role": "user",
     "content": input_text}
]

# Apply the chat template
prompt = tokenizer.apply_chat_template(conversation=dialogue_template,
                                       tokenize=False, # keep as raw text (not tokenized)
                                       add_generation_prompt=True)
print(f"\nPrompt (formatted):\n{prompt}")

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Tokenize the input text (turn it into numbers) and send it to GPU
# input_ids = tokenizer(prompt, return_tensors="pt").to("cuda")
# print(f"Model input (tokenized):\n{input_ids}\n")
# 
# # Generate outputs passed on the tokenized input
# # See generate docs: https://huggingface.co/docs/transformers/v4.38.2/en/main_classes/text_generation#transformers.GenerationConfig
# outputs = llm_model.generate(**input_ids,
#                              max_new_tokens=256) # define the maximum number of new tokens to create
# 
# print(f"Model output (tokens):\n{outputs[0]}\n")

# Decode the output tokens to text
outputs_decoded = tokenizer.decode(outputs[0])
print(f"Model output (decoded):\n{outputs_decoded}\n")

print(f"Input text: {input_text}\n")
print(f"Output text:\n{outputs_decoded.replace(prompt, '').replace('<bos>', '').replace('<eos>', '')}")

# Python Data Science questions generated with GPT4
gpt4_questions = [
    "Chuyên ngành của trường gồm những gì",
    "Điểm để vào được trường",
]


# Manually created question list
manual_questions = [
    "Trường có kí túc xá không",
    "Trường có ưu đãi học bổng không",
    "Cơ sở vật chất của trường",
]

query_list = gpt4_questions + manual_questions

import random
query = random.choice(query_list)

print(f"Query: {query}")

# Get just the scores and indices of top related results
scores, indices = retrieve_relevant_resources(query=query,
                                              embeddings=embeddings)
scores, indices

def prompt_formatter(query: str,
                     context_items: list[dict]) -> str:
    """
    Augments query with text-based context from context_items.
    """
    # Join context items into one dotted paragraph
    context = "- " + "\n- ".join([item["sentence_chunk"] for item in context_items])

    # Create a base prompt with examples to help the model
    # Note: this is very customizable, I've chosen to use 3 examples of the answer style we'd like.
    # We could also write this in a txt file and import it in if we wanted.
    base_prompt = """
      Based on the following context items, please answer the query.
      Give yourself room to think by extracting relevant passages from the context before answering the query.
      Don't return the thinking, only return the answer.
      Make sure your answers are as explanatory as possible.

      User query: {query}
      Answer:
    """


    # Update base prompt with context items and query
    base_prompt = base_prompt.format(context=context, query=query)

    # Create prompt template for instruction-tuned model
    dialogue_template = [
        {"role": "user",
        "content": base_prompt}
    ]

    # Apply the chat template
    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,
                                          tokenize=False,
                                          add_generation_prompt=True)
    return prompt

query = random.choice(query_list)
print(f"Query: {query}")

# Get relevant resources
scores, indices = retrieve_relevant_resources(query=query,
                                              embeddings=embeddings)

# Create a list of context items
context_items = [pages_and_chunks[i] for i in indices]

# Format prompt with context items
prompt = prompt_formatter(query=query,
                          context_items=context_items)
print(prompt)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# input_ids = tokenizer(prompt, return_tensors="pt").to("cuda")
# 
# # Generate an output of tokens
# outputs = llm_model.generate(**input_ids,
#                              temperature=0.7, # lower temperature = more deterministic outputs, higher temperature = more creative outputs
#                              do_sample=True, # whether or not to use sampling, see https://huyenchip.com/2024/01/16/sampling.html for more
#                              max_new_tokens=256) # how many new tokens to generate from prompt
# 
# # Turn the output tokens into text
# output_text = tokenizer.decode(outputs[0])
# 
# print(f"Query: {query}")
# print(f"RAG answer:\n{output_text.replace(prompt, '')}")

def ask(query,
        temperature=0.7,
        max_new_tokens=512,
        format_answer_text=True,
        return_answer_only=True):
    """
    Takes a query, finds relevant resources/context and generates an answer to the query based on the relevant resources.
    """

    # Get just the scores and indices of top related results
    scores, indices = retrieve_relevant_resources(query=query,
                                                  embeddings=embeddings)

    # Create a list of context items
    context_items = [pages_and_chunks[i] for i in indices]

    # Add score to context item
    for i, item in enumerate(context_items):
        item["score"] = scores[i].cpu() # return score back to CPU

    # Format the prompt with context items
    prompt = prompt_formatter(query=query,
                              context_items=context_items)

    # Tokenize the prompt
    input_ids = tokenizer(prompt, return_tensors="pt").to("cuda")

    # Generate an output of tokens
    outputs = llm_model.generate(**input_ids,
                                 temperature=temperature,
                                 do_sample=True,
                                 max_new_tokens=max_new_tokens)

    # Turn the output tokens into text
    output_text = tokenizer.decode(outputs[0])

    if format_answer_text:
        # Replace special tokens and unnecessary help message
        output_text = output_text.replace(prompt, "").replace("<bos>", "").replace("<eos>", "").replace("Sure, here is the answer to the user query:\n\n", "")

    # Only return the answer without the context items
    if return_answer_only:
        return output_text

    return output_text, context_items

query = random.choice(query_list)
print(f"Query: {query}")

# Answer query with context and return context
answer, context_items = ask(query=query,
                            temperature=0.7,
                            max_new_tokens=512,
                            return_answer_only=False)

print(f"Answer:\n")
print_wrapped(answer)
print(f"Context items:")
context_items

"""## Host Pipeline bằng Flask API"""

!pip install Flask

"""## Chạy Flask App"""

from flask import Flask, request, jsonify
import torch
from sentence_transformers import SentenceTransformer, util
from transformers import AutoTokenizer, AutoModelForCausalLM

# Initialize Flask app
app = Flask(__name__)

# # Load my  models (make sure they are the same as in my  RAG pipeline)
# embedding_model = SentenceTransformer("all-mpnet-base-v2")
# tokenizer = AutoTokenizer.from_pretrained("ricepaper/vi-gemma-2b-RAG")
# llm_model = AutoModelForCausalLM.from_pretrained("ricepaper/vi-gemma-2b-RAG", torch_dtype=torch.float16)
# llm_model.to("cuda")

# # Load my  embeddings (make sure they are the same as in my  RAG pipeline)
# embeddings = torch.load("embeddings.pt")

def retrieve_relevant_resources(query: str, n_resources_to_return: int = 5):
    query_embedding = embedding_model.encode(query, convert_to_tensor=True)
    dot_scores = util.dot_score(query_embedding, embeddings)[0]
    scores, indices = torch.topk(input=dot_scores, k=n_resources_to_return)
    return scores, indices

def prompt_formatter(query: str, context_items: list):
    context = "- " + "\n- ".join([item["sentence_chunk"] for item in context_items])
    base_prompt = f"""
    Based on the following context items, please answer the query.
    User query: {query}
    Context:
    {context}
    Answer:"""
    return base_prompt

@app.route('/generate', methods=['POST'])
def generate():
    data = request.json
    query = data.get('query', '')

    # Retrieve relevant resources
    scores, indices = retrieve_relevant_resources(query)

    # Create a list of context items
    context_items = [pages_and_chunks[i] for i in indices]

    # Format the prompt with context items
    prompt = prompt_formatter(query=query, context_items=context_items)

    # Tokenize and generate the response
    input_ids = tokenizer(prompt, return_tensors="pt").to("cuda")
    outputs = llm_model.generate(**input_ids, max_new_tokens=256)
    output_text = tokenizer.decode(outputs[0])

    # Clean up the response and return it
    response_text = output_text.replace(prompt, "").strip()
    return jsonify({'query': query, 'response': response_text})

!pip install flask-ngrok

!pip install pyngrok
from pyngrok import ngrok

# Get your authtoken from https://dashboard.ngrok.com/get-started/your-authtoken
ngrok.set_auth_token("2v9nlPnspNR4vi2fOY8kaTZBSK0_6c6RNqzfLQBJWUtcoqMEF")

# Start ngrok
public_url = ngrok.connect(8080)
print(f"Public URL: {public_url}")

# Run your Flask app
app.run()

# default: curl -X POST http://<your-ngrok-url>.ngrok.io/generate -H "Content-Type: application/json" -d '{"query": "What is linear regression?"}'
# curl -X POST http://127.0.0.1:5000.ngrok.io/generate -H "Content-Type: application/json" -d '{"query": "What is linear regression?"}'



"""# Phiên bản ngắn gọn hơn

"""

from huggingface_hub import notebook_login

# Log in to Hugging Face (replace 'YOUR_TOKEN' with your actual token)
notebook_login()

# Cài đặt các thư viện
!pip install PyMuPDF sentence-transformers torch transformers accelerate numpy

# Kết nối Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Đọc file PDF
import fitz
pdf_path = '/content/drive/MyDrive/AI_DATA/Thông Tin Tuyển Sinh.pdf'  # Thay bằng đường dẫn của bạn
doc = fitz.open(pdf_path)
text = ""
for page in doc:
    text += page.get_text()
doc.close()

# Phân đoạn văn bản
def chunk_text(text, chunk_size=500):
    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]

chunks = chunk_text(text)

# Nhúng văn bản
from sentence_transformers import SentenceTransformer
import numpy as np
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = embedding_model.encode(chunks, convert_to_tensor=True).cpu().numpy()

# Truy xuất vector
from numpy.linalg import norm
def cosine_similarity(vec1, vec2):
    return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2))

def retrieve_relevant_chunks(query, embeddings, chunks, top_k=3):
    query_embedding = embedding_model.encode(query, convert_to_tensor=True).cpu().numpy()
    similarities = [cosine_similarity(query_embedding, emb) for emb in embeddings]
    top_indices = np.argsort(similarities)[-top_k:][::-1]
    return [chunks[i] for i in top_indices], [similarities[i] for i in top_indices]

# Tải LLM
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
model_name = "ura-hcmut/GemSUra-2B"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", torch_dtype=torch.float16)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Sinh câu trả lời
def generate_response(query, chunks, similarities):
    context = "\n".join([f"Đoạn {i+1} (độ tương đồng: {sim:.2f}): {chunk}" for i, (chunk, sim) in enumerate(zip(chunks, similarities))])
    prompt = f"""Bạn là trợ lý tuyển sinh đại học. Dựa trên thông tin sau từ tài liệu tuyển sinh, hãy trả lời câu hỏi: "{query}"

Thông tin:
{context}

Câu trả lời: """
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    outputs = model.generate(**inputs, max_new_tokens=150, temperature=0.7, do_sample=True)
    return tokenizer.decode(outputs[0], skip_special_tokens=True).split("Câu trả lời:")[-1].strip()

# Thử nghiệm
query = "Học phí của trường bao nhiêu một năm?"
relevant_chunks, similarities = retrieve_relevant_chunks(query, embeddings, chunks)
response = generate_response(query, relevant_chunks, similarities)
print(f"Truy vấn: {query}")
print(f"Câu trả lời: {response}")

# Thử nghiệm
query = "Trường có quy định gì về học bổng"
relevant_chunks, similarities = retrieve_relevant_chunks(query, embeddings, chunks)
response = generate_response(query, relevant_chunks, similarities)
print(f"Truy vấn: {query}")
print(f"Câu trả lời: {response}")